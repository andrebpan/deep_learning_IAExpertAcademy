{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.read_csv('datasets/petr4_treinamento.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.718563</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>15.970000</td>\n",
       "      <td>15.938125</td>\n",
       "      <td>22173100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>16.139999</td>\n",
       "      <td>15.980000</td>\n",
       "      <td>16.049999</td>\n",
       "      <td>16.017963</td>\n",
       "      <td>23552200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>2017-12-28</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.129999</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>19011500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1242 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Open       High        Low      Close  Adj Close  \\\n",
       "0     2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1     2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2     2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3     2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4     2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "1240  2017-12-25  15.750000  15.750000  15.750000  15.750000  15.718563   \n",
       "1241  2017-12-26  15.750000  15.990000  15.690000  15.970000  15.938125   \n",
       "1242  2017-12-27  15.990000  16.139999  15.980000  16.049999  16.017963   \n",
       "1243  2017-12-28  16.100000  16.129999  16.000000  16.100000  16.067865   \n",
       "1244  2017-12-29  16.100000  16.100000  16.100000  16.100000  16.067865   \n",
       "\n",
       "          Volume  \n",
       "0     30182600.0  \n",
       "1     30552600.0  \n",
       "2     36141000.0  \n",
       "3     28069600.0  \n",
       "4     29091300.0  \n",
       "...          ...  \n",
       "1240         0.0  \n",
       "1241  22173100.0  \n",
       "1242  23552200.0  \n",
       "1243  19011500.0  \n",
       "1244         0.0  \n",
       "\n",
       "[1242 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = base.dropna()\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treinamento = base.iloc[: , 1:7].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9990000e+01, 2.0209999e+01, 1.9690001e+01, 1.9690001e+01,\n",
       "        1.8086271e+01, 3.0182600e+07],\n",
       "       [1.9809999e+01, 2.0400000e+01, 1.9700001e+01, 2.0400000e+01,\n",
       "        1.8738441e+01, 3.0552600e+07],\n",
       "       [2.0330000e+01, 2.0620001e+01, 2.0170000e+01, 2.0430000e+01,\n",
       "        1.8766001e+01, 3.6141000e+07],\n",
       "       ...,\n",
       "       [1.5990000e+01, 1.6139999e+01, 1.5980000e+01, 1.6049999e+01,\n",
       "        1.6017963e+01, 2.3552200e+07],\n",
       "       [1.6100000e+01, 1.6129999e+01, 1.6000000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 1.9011500e+07],\n",
       "       [1.6100000e+01, 1.6100000e+01, 1.6100000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 0.0000000e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizador = MinMaxScaler(feature_range=(0,1))\n",
    "base_treinamento_normalizada = normalizador.fit_transform(base_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938],\n",
       "       [0.7562984 ],\n",
       "       [0.78149225],\n",
       "       ...,\n",
       "       [0.57122093],\n",
       "       [0.57655039],\n",
       "       [0.57655039]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizador_previsao = MinMaxScaler(feature_range=(0,1))\n",
    "normalizador_previsao.fit_transform(base_treinamento[: ,0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938, 0.77266112, 0.79682707, 0.76080559, 0.6838135 ,\n",
       "        0.04318274],\n",
       "       [0.7562984 , 0.78187106, 0.79733884, 0.79567784, 0.71590949,\n",
       "        0.0437121 ],\n",
       "       [0.78149225, 0.79253519, 0.82139202, 0.79715132, 0.71726583,\n",
       "        0.05170752],\n",
       "       ...,\n",
       "       [0.57122093, 0.57537562, 0.60696008, 0.58202356, 0.58202349,\n",
       "        0.03369652],\n",
       "       [0.57655039, 0.57489089, 0.60798362, 0.5844794 , 0.58447937,\n",
       "        0.02720006],\n",
       "       [0.57655039, 0.57343674, 0.61310133, 0.5844794 , 0.58447937,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_treinamento_normalizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] #previsores\n",
    "y = [] #preço real\n",
    "for i in range(90, 1242): #90 preços anteriores\n",
    "    X.append(base_treinamento_normalizada[i-90: i, 0:6])\n",
    "    y.append(base_treinamento_normalizada[i,0])\n",
    "    \n",
    "X, y = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938, 0.77266112, 0.79682707, 0.76080559, 0.6838135 ,\n",
       "        0.04318274],\n",
       "       [0.7562984 , 0.78187106, 0.79733884, 0.79567784, 0.71590949,\n",
       "        0.0437121 ],\n",
       "       [0.78149225, 0.79253519, 0.82139202, 0.79715132, 0.71726583,\n",
       "        0.05170752],\n",
       "       [0.78875969, 0.7949588 , 0.81013311, 0.77996075, 0.70144373,\n",
       "        0.04015963],\n",
       "       [0.77083338, 0.77363063, 0.78505624, 0.75147351, 0.67522435,\n",
       "        0.0416214 ],\n",
       "       [0.74806197, 0.75618037, 0.78505624, 0.76031438, 0.68336137,\n",
       "        0.03485382],\n",
       "       [0.75436047, 0.76490543, 0.78915051, 0.76768177, 0.69014234,\n",
       "        0.02507502],\n",
       "       [0.75823643, 0.76442079, 0.79733884, 0.77013751, 0.6924025 ,\n",
       "        0.0260728 ],\n",
       "       [0.76598837, 0.77411537, 0.79682707, 0.76227897, 0.68516964,\n",
       "        0.0404927 ],\n",
       "       [0.76598837, 0.77411537, 0.79682707, 0.76719061, 0.68969016,\n",
       "        0.0423977 ],\n",
       "       [0.76017437, 0.75714973, 0.79222108, 0.76817293, 0.69059437,\n",
       "        0.02401858],\n",
       "       [0.75872098, 0.75908871, 0.79222108, 0.76178781, 0.68471746,\n",
       "        0.02821315],\n",
       "       [0.75581391, 0.75714973, 0.78915051, 0.75540279, 0.6788408 ,\n",
       "        0.02706042],\n",
       "       [0.74467054, 0.74309258, 0.77533265, 0.74607071, 0.67025175,\n",
       "        0.02587622],\n",
       "       [0.7374031 , 0.74357736, 0.77328557, 0.75540279, 0.6788408 ,\n",
       "        0.03367205],\n",
       "       [0.7374031 , 0.74454673, 0.77328557, 0.75392926, 0.67748471,\n",
       "        0.02460946],\n",
       "       [0.73498067, 0.75036355, 0.78045041, 0.75687631, 0.68019705,\n",
       "        0.02806007],\n",
       "       [0.75242248, 0.75327189, 0.77533265, 0.74508849, 0.66934774,\n",
       "        0.02878973],\n",
       "       [0.73401163, 0.73194382, 0.75332651, 0.73231836, 0.65759427,\n",
       "        0.03876941],\n",
       "       [0.71656977, 0.71352399, 0.71903787, 0.68762287, 0.6164569 ,\n",
       "        0.09583767],\n",
       "       [0.68120155, 0.68153175, 0.70522006, 0.68172891, 0.61103237,\n",
       "        0.04756616],\n",
       "       [0.67538755, 0.69704314, 0.71647907, 0.70039291, 0.62821037,\n",
       "        0.04129104],\n",
       "       [0.67635659, 0.68250121, 0.70470824, 0.67779964, 0.60741587,\n",
       "        0.04620398],\n",
       "       [0.63372098, 0.67959287, 0.67246673, 0.68172891, 0.61103237,\n",
       "        0.11064144],\n",
       "       [0.66521318, 0.66553563, 0.6862846 , 0.65815327, 0.58933361,\n",
       "        0.04418925],\n",
       "       [0.65649225, 0.66456617, 0.67553736, 0.65324168, 0.584813  ,\n",
       "        0.0530315 ],\n",
       "       [0.64680228, 0.65487159, 0.67860793, 0.6650295 , 0.5956623 ,\n",
       "        0.04444964],\n",
       "       [0.66618222, 0.66553563, 0.69651996, 0.66797641, 0.59837464,\n",
       "        0.03194532],\n",
       "       [0.65843028, 0.66068832, 0.6888434 , 0.66159139, 0.59249793,\n",
       "        0.0370597 ],\n",
       "       [0.64970935, 0.65535633, 0.6862846 , 0.6596267 , 0.59068976,\n",
       "        0.0357702 ],\n",
       "       [0.65116274, 0.66311202, 0.68577277, 0.67288805, 0.60289526,\n",
       "        0.02903152],\n",
       "       [0.66424419, 0.67426079, 0.70470824, 0.68271123, 0.61193639,\n",
       "        0.0412361 ],\n",
       "       [0.67344961, 0.67038294, 0.68730803, 0.65913564, 0.59023768,\n",
       "        0.03711206],\n",
       "       [0.64292631, 0.6446922 , 0.66939616, 0.64440082, 0.57667593,\n",
       "        0.04346845],\n",
       "       [0.64486434, 0.64178381, 0.65967247, 0.63605111, 0.56899095,\n",
       "        0.04421171],\n",
       "       [0.62257747, 0.62190984, 0.65148414, 0.62622798, 0.55994986,\n",
       "        0.04364257],\n",
       "       [0.60949617, 0.61027635, 0.63510752, 0.61591359, 0.55045665,\n",
       "        0.04779322],\n",
       "       [0.60998067, 0.61609307, 0.6407369 , 0.61935165, 0.55362107,\n",
       "        0.04092922],\n",
       "       [0.60852713, 0.60979157, 0.63613096, 0.60952857, 0.54457989,\n",
       "        0.03981569],\n",
       "       [0.59593023, 0.61803199, 0.62845445, 0.62377213, 0.55768961,\n",
       "        0.04509603],\n",
       "       [0.61143411, 0.62190984, 0.63254862, 0.60412577, 0.5396073 ,\n",
       "        0.05085238],\n",
       "       [0.60222863, 0.60542899, 0.6320368 , 0.60707267, 0.54231954,\n",
       "        0.04531064],\n",
       "       [0.64922481, 0.67862336, 0.6704196 , 0.68025539, 0.60967603,\n",
       "        0.10572707],\n",
       "       [0.68362398, 0.74212312, 0.72620261, 0.72445981, 0.65036132,\n",
       "        0.08930445],\n",
       "       [0.70687989, 0.72952012, 0.7185261 , 0.69597258, 0.62414194,\n",
       "        0.04376518],\n",
       "       [0.68265509, 0.71255448, 0.7062436 , 0.72347744, 0.64945705,\n",
       "        0.03589495],\n",
       "       [0.70978682, 0.72079491, 0.74257927, 0.71414542, 0.64086801,\n",
       "        0.03739277],\n",
       "       [0.70784879, 0.72370339, 0.74769703, 0.71463658, 0.64132019,\n",
       "        0.04530406],\n",
       "       [0.71608527, 0.73242845, 0.74104401, 0.74115922, 0.66573124,\n",
       "        0.03887614],\n",
       "       [0.73643411, 0.74066888, 0.76202661, 0.73133599, 0.65669001,\n",
       "        0.06269313],\n",
       "       [0.7122093 , 0.73097431, 0.75332651, 0.73673879, 0.6616627 ,\n",
       "        0.05787405],\n",
       "       [0.7122093 , 0.73097431, 0.75281474, 0.73182715, 0.65714209,\n",
       "        0.04839097],\n",
       "       [0.7194767 , 0.72176442, 0.74513818, 0.71954817, 0.6458407 ,\n",
       "        0.03954013],\n",
       "       [0.70348832, 0.70722254, 0.73541453, 0.70383112, 0.63137489,\n",
       "        0.03144514],\n",
       "       [0.69525189, 0.69995148, 0.73387917, 0.70874262, 0.63589531,\n",
       "        0.02308847],\n",
       "       [0.70397287, 0.70528357, 0.73183214, 0.70677803, 0.63408723,\n",
       "        0.03482392],\n",
       "       [0.70397287, 0.7081919 , 0.73490276, 0.70677803, 0.63408723,\n",
       "        0.02257928],\n",
       "       [0.69767442, 0.69510427, 0.72824974, 0.69842833, 0.6264022 ,\n",
       "        0.01903582],\n",
       "       [0.68168605, 0.68395536, 0.71136131, 0.67927317, 0.60877212,\n",
       "        0.02224034],\n",
       "       [0.68168605, 0.68395536, 0.69344933, 0.66306491, 0.59385423,\n",
       "        0.02942397],\n",
       "       [0.65310078, 0.66650509, 0.69396111, 0.67779964, 0.60741587,\n",
       "        0.02244093],\n",
       "       [0.66618222, 0.67571493, 0.6949847 , 0.66355598, 0.59430621,\n",
       "        0.02782257],\n",
       "       [0.64825581, 0.66117305, 0.68730803, 0.66797641, 0.59837464,\n",
       "        0.02440802],\n",
       "       [0.66182175, 0.66117305, 0.6765609 , 0.64685666, 0.57893629,\n",
       "        0.03144357],\n",
       "       [0.64341085, 0.6776539 , 0.68372569, 0.68516703, 0.61419665,\n",
       "        0.04400526],\n",
       "       [0.67877902, 0.69704314, 0.71903787, 0.69842833, 0.6264022 ,\n",
       "        0.04546845],\n",
       "       [0.69137592, 0.69122642, 0.7036848 , 0.67730848, 0.60696374,\n",
       "        0.03177292],\n",
       "       [0.66569772, 0.66941348, 0.6862846 , 0.67583495, 0.6056075 ,\n",
       "        0.03919891],\n",
       "       [0.65406982, 0.6572952 , 0.665302  , 0.63998039, 0.57260735,\n",
       "        0.05120333],\n",
       "       [0.64292631, 0.65341735, 0.68116684, 0.66306491, 0.59385423,\n",
       "        0.03397579],\n",
       "       [0.64147292, 0.64614639, 0.65813715, 0.63703343, 0.56989516,\n",
       "        0.05635362],\n",
       "       [0.63565891, 0.66262729, 0.665302  , 0.66895878, 0.5992789 ,\n",
       "        0.04077971],\n",
       "       [0.67587209, 0.68880271, 0.70777897, 0.6969548 , 0.625046  ,\n",
       "        0.0548714 ],\n",
       "       [0.68653106, 0.70382942, 0.71903787, 0.71660126, 0.64312846,\n",
       "        0.03461346],\n",
       "       [0.70300383, 0.73921474, 0.74411464, 0.73280952, 0.6580463 ,\n",
       "        0.04969664],\n",
       "       [0.71996119, 0.74600097, 0.76202661, 0.74852661, 0.67251211,\n",
       "        0.04766145],\n",
       "       [0.73982553, 0.74745521, 0.76867958, 0.73526526, 0.66030651,\n",
       "        0.05031056],\n",
       "       [0.76550388, 0.79059622, 0.80962134, 0.79666016, 0.71681365,\n",
       "        0.10120858],\n",
       "       [0.74854651, 0.76732913, 0.7840328 , 0.7804519 , 0.71911682,\n",
       "        0.06567045],\n",
       "       [0.75823643, 0.79301983, 0.80501535, 0.78831045, 0.72648688,\n",
       "        0.04828195],\n",
       "       [0.78924419, 0.79447407, 0.80706238, 0.77455795, 0.71358928,\n",
       "        0.06152981],\n",
       "       [0.76598837, 0.78041692, 0.80348004, 0.79223972, 0.73017189,\n",
       "        0.04455508],\n",
       "       [0.78488372, 0.79835191, 0.82702155, 0.80648339, 0.74353023,\n",
       "        0.03775975],\n",
       "       [0.80184109, 0.80222976, 0.82395082, 0.79027514, 0.72832946,\n",
       "        0.03492235],\n",
       "       [0.77761628, 0.78768783, 0.81729785, 0.7907662 , 0.72879006,\n",
       "        0.03271233],\n",
       "       [0.77325581, 0.78138628, 0.79785051, 0.77406679, 0.71312854,\n",
       "        0.0315204 ],\n",
       "       [0.7562984 , 0.75521086, 0.78096208, 0.75098236, 0.69147899,\n",
       "        0.03087142],\n",
       "       [0.74273261, 0.74697043, 0.77430911, 0.75392926, 0.69424286,\n",
       "        0.04384244],\n",
       "       [0.74127907, 0.74503146, 0.77840328, 0.75491163, 0.6951641 ,\n",
       "        0.03128876],\n",
       "       [0.74224806, 0.76635967, 0.78505624, 0.76375249, 0.7034554 ,\n",
       "        0.03586405]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 90, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">42,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m42,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m30,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">113,451</span> (443.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m113,451\u001b[0m (443.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">113,451</span> (443.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m113,451\u001b[0m (443.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 100, return_sequences=True, input_shape=(X.shape[1], 6)))#x.shape[1],1 enviando 90 colunas para a rede neural, apenas a coluna com valor de abertura\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units=50))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.compile(optimizer='rmsprop', loss = 'mean_squared_error', metrics = ['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', min_delta=1e-10, patience=10, verbose=True)\n",
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='pesos.keras', monitor='loss', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0591 - mean_absolute_error: 0.1752\n",
      "Epoch 1: loss improved from inf to 0.02900, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 62ms/step - loss: 0.0583 - mean_absolute_error: 0.1737 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0149 - mean_absolute_error: 0.0958\n",
      "Epoch 2: loss improved from 0.02900 to 0.01395, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.0148 - mean_absolute_error: 0.0957 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0122 - mean_absolute_error: 0.0855\n",
      "Epoch 3: loss improved from 0.01395 to 0.01315, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 0.0122 - mean_absolute_error: 0.0856 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0121 - mean_absolute_error: 0.0834\n",
      "Epoch 4: loss improved from 0.01315 to 0.01222, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0121 - mean_absolute_error: 0.0835 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0100 - mean_absolute_error: 0.0781\n",
      "Epoch 5: loss improved from 0.01222 to 0.01026, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0100 - mean_absolute_error: 0.0781 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0093 - mean_absolute_error: 0.0751\n",
      "Epoch 6: loss improved from 0.01026 to 0.00939, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 0.0093 - mean_absolute_error: 0.0751 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0088 - mean_absolute_error: 0.0727\n",
      "Epoch 7: loss improved from 0.00939 to 0.00830, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0088 - mean_absolute_error: 0.0726 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0077 - mean_absolute_error: 0.0669\n",
      "Epoch 8: loss did not improve from 0.00830\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0077 - mean_absolute_error: 0.0670 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0069 - mean_absolute_error: 0.0636\n",
      "Epoch 9: loss improved from 0.00830 to 0.00744, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 0.0070 - mean_absolute_error: 0.0637 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0089 - mean_absolute_error: 0.0724\n",
      "Epoch 10: loss improved from 0.00744 to 0.00727, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0088 - mean_absolute_error: 0.0722 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0072 - mean_absolute_error: 0.0645\n",
      "Epoch 11: loss improved from 0.00727 to 0.00682, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0072 - mean_absolute_error: 0.0645 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0067 - mean_absolute_error: 0.0631\n",
      "Epoch 12: loss did not improve from 0.00682\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0068 - mean_absolute_error: 0.0632 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0059 - mean_absolute_error: 0.0588\n",
      "Epoch 13: loss improved from 0.00682 to 0.00673, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0059 - mean_absolute_error: 0.0588 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0059 - mean_absolute_error: 0.0580\n",
      "Epoch 14: loss improved from 0.00673 to 0.00518, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 0.0059 - mean_absolute_error: 0.0579 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0072 - mean_absolute_error: 0.0644\n",
      "Epoch 15: loss did not improve from 0.00518\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0071 - mean_absolute_error: 0.0642 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0049 - mean_absolute_error: 0.0540\n",
      "Epoch 16: loss improved from 0.00518 to 0.00513, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.0049 - mean_absolute_error: 0.0540 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0056 - mean_absolute_error: 0.0563\n",
      "Epoch 17: loss did not improve from 0.00513\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.0056 - mean_absolute_error: 0.0563 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0043 - mean_absolute_error: 0.0506\n",
      "Epoch 18: loss improved from 0.00513 to 0.00482, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 0.0043 - mean_absolute_error: 0.0507 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0052 - mean_absolute_error: 0.0553\n",
      "Epoch 19: loss did not improve from 0.00482\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.0052 - mean_absolute_error: 0.0553 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0057 - mean_absolute_error: 0.0536\n",
      "Epoch 20: loss improved from 0.00482 to 0.00467, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0057 - mean_absolute_error: 0.0535 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0041 - mean_absolute_error: 0.0489\n",
      "Epoch 21: loss improved from 0.00467 to 0.00433, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0041 - mean_absolute_error: 0.0490 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0055 - mean_absolute_error: 0.0553\n",
      "Epoch 22: loss did not improve from 0.00433\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 0.0055 - mean_absolute_error: 0.0552 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0052 - mean_absolute_error: 0.0557\n",
      "Epoch 23: loss did not improve from 0.00433\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.0052 - mean_absolute_error: 0.0556 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0042 - mean_absolute_error: 0.0492\n",
      "Epoch 24: loss improved from 0.00433 to 0.00421, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 0.0042 - mean_absolute_error: 0.0492 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0043 - mean_absolute_error: 0.0505\n",
      "Epoch 25: loss improved from 0.00421 to 0.00417, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - loss: 0.0043 - mean_absolute_error: 0.0504 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0046 - mean_absolute_error: 0.0524\n",
      "Epoch 26: loss did not improve from 0.00417\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - loss: 0.0046 - mean_absolute_error: 0.0523 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0046 - mean_absolute_error: 0.0519\n",
      "Epoch 27: loss did not improve from 0.00417\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 0.0046 - mean_absolute_error: 0.0518 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0036 - mean_absolute_error: 0.0463\n",
      "Epoch 28: loss improved from 0.00417 to 0.00408, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.0036 - mean_absolute_error: 0.0463 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0036 - mean_absolute_error: 0.0451\n",
      "Epoch 29: loss improved from 0.00408 to 0.00386, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - loss: 0.0037 - mean_absolute_error: 0.0452 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0038 - mean_absolute_error: 0.0473\n",
      "Epoch 30: loss improved from 0.00386 to 0.00361, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.0038 - mean_absolute_error: 0.0472 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0037 - mean_absolute_error: 0.0462\n",
      "Epoch 31: loss did not improve from 0.00361\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0037 - mean_absolute_error: 0.0462 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0030 - mean_absolute_error: 0.0418\n",
      "Epoch 32: loss improved from 0.00361 to 0.00320, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0030 - mean_absolute_error: 0.0418 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0036 - mean_absolute_error: 0.0443\n",
      "Epoch 33: loss did not improve from 0.00320\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0036 - mean_absolute_error: 0.0443 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0034 - mean_absolute_error: 0.0437\n",
      "Epoch 34: loss did not improve from 0.00320\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.0034 - mean_absolute_error: 0.0438 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0030 - mean_absolute_error: 0.0409\n",
      "Epoch 35: loss did not improve from 0.00320\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0030 - mean_absolute_error: 0.0409 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0035 - mean_absolute_error: 0.0443\n",
      "Epoch 36: loss did not improve from 0.00320\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0035 - mean_absolute_error: 0.0443 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0027 - mean_absolute_error: 0.0399\n",
      "Epoch 37: loss improved from 0.00320 to 0.00304, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.0028 - mean_absolute_error: 0.0399 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0026 - mean_absolute_error: 0.0384\n",
      "Epoch 38: loss improved from 0.00304 to 0.00283, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0026 - mean_absolute_error: 0.0384 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0031 - mean_absolute_error: 0.0409\n",
      "Epoch 39: loss did not improve from 0.00283\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.0031 - mean_absolute_error: 0.0410 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0031 - mean_absolute_error: 0.0413\n",
      "Epoch 40: loss did not improve from 0.00283\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.0031 - mean_absolute_error: 0.0413 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0031 - mean_absolute_error: 0.0413\n",
      "Epoch 41: loss did not improve from 0.00283\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0031 - mean_absolute_error: 0.0413 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0029 - mean_absolute_error: 0.0397\n",
      "Epoch 42: loss did not improve from 0.00283\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0029 - mean_absolute_error: 0.0398 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0028 - mean_absolute_error: 0.0391\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 43: loss improved from 0.00283 to 0.00274, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0028 - mean_absolute_error: 0.0390 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0024 - mean_absolute_error: 0.0371\n",
      "Epoch 44: loss improved from 0.00274 to 0.00236, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0024 - mean_absolute_error: 0.0371 - learning_rate: 2.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0023 - mean_absolute_error: 0.0362\n",
      "Epoch 45: loss improved from 0.00236 to 0.00216, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0023 - mean_absolute_error: 0.0362 - learning_rate: 2.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0020 - mean_absolute_error: 0.0338\n",
      "Epoch 46: loss did not improve from 0.00216\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0020 - mean_absolute_error: 0.0339 - learning_rate: 2.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0023 - mean_absolute_error: 0.0350\n",
      "Epoch 47: loss did not improve from 0.00216\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.0023 - mean_absolute_error: 0.0351 - learning_rate: 2.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0022 - mean_absolute_error: 0.0359\n",
      "Epoch 48: loss did not improve from 0.00216\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.0022 - mean_absolute_error: 0.0360 - learning_rate: 2.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0023 - mean_absolute_error: 0.0367\n",
      "Epoch 49: loss did not improve from 0.00216\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0023 - mean_absolute_error: 0.0367 - learning_rate: 2.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0022 - mean_absolute_error: 0.0356\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 50: loss did not improve from 0.00216\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0022 - mean_absolute_error: 0.0356 - learning_rate: 2.0000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0021 - mean_absolute_error: 0.0338\n",
      "Epoch 51: loss did not improve from 0.00216\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.0021 - mean_absolute_error: 0.0338 - learning_rate: 4.0000e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0022 - mean_absolute_error: 0.0359\n",
      "Epoch 52: loss improved from 0.00216 to 0.00214, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0022 - mean_absolute_error: 0.0359 - learning_rate: 4.0000e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0023 - mean_absolute_error: 0.0354\n",
      "Epoch 53: loss did not improve from 0.00214\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0023 - mean_absolute_error: 0.0354 - learning_rate: 4.0000e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0023 - mean_absolute_error: 0.0360\n",
      "Epoch 54: loss did not improve from 0.00214\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0022 - mean_absolute_error: 0.0359 - learning_rate: 4.0000e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0021 - mean_absolute_error: 0.0330\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 55: loss improved from 0.00214 to 0.00213, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0021 - mean_absolute_error: 0.0330 - learning_rate: 4.0000e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0020 - mean_absolute_error: 0.0332\n",
      "Epoch 56: loss did not improve from 0.00213\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0020 - mean_absolute_error: 0.0333 - learning_rate: 8.0000e-06\n",
      "Epoch 57/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0020 - mean_absolute_error: 0.0344\n",
      "Epoch 57: loss did not improve from 0.00213\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0020 - mean_absolute_error: 0.0344 - learning_rate: 8.0000e-06\n",
      "Epoch 58/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0020 - mean_absolute_error: 0.0337\n",
      "Epoch 58: loss improved from 0.00213 to 0.00204, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0020 - mean_absolute_error: 0.0338 - learning_rate: 8.0000e-06\n",
      "Epoch 59/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0022 - mean_absolute_error: 0.0361\n",
      "Epoch 59: loss did not improve from 0.00204\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0022 - mean_absolute_error: 0.0361 - learning_rate: 8.0000e-06\n",
      "Epoch 60/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0021 - mean_absolute_error: 0.0357\n",
      "Epoch 60: loss did not improve from 0.00204\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0021 - mean_absolute_error: 0.0357 - learning_rate: 8.0000e-06\n",
      "Epoch 61/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0023 - mean_absolute_error: 0.0357\n",
      "Epoch 61: loss did not improve from 0.00204\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0023 - mean_absolute_error: 0.0357 - learning_rate: 8.0000e-06\n",
      "Epoch 62/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0022 - mean_absolute_error: 0.0357\n",
      "Epoch 62: loss did not improve from 0.00204\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0022 - mean_absolute_error: 0.0357 - learning_rate: 8.0000e-06\n",
      "Epoch 63/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0023 - mean_absolute_error: 0.0352\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 63: loss did not improve from 0.00204\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0023 - mean_absolute_error: 0.0352 - learning_rate: 8.0000e-06\n",
      "Epoch 64/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0024 - mean_absolute_error: 0.0370\n",
      "Epoch 64: loss did not improve from 0.00204\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0024 - mean_absolute_error: 0.0370 - learning_rate: 1.6000e-06\n",
      "Epoch 65/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0024 - mean_absolute_error: 0.0362\n",
      "Epoch 65: loss did not improve from 0.00204\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.0024 - mean_absolute_error: 0.0362 - learning_rate: 1.6000e-06\n",
      "Epoch 66/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0021 - mean_absolute_error: 0.0351\n",
      "Epoch 66: loss did not improve from 0.00204\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0021 - mean_absolute_error: 0.0351 - learning_rate: 1.6000e-06\n",
      "Epoch 67/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0021 - mean_absolute_error: 0.0353\n",
      "Epoch 67: loss did not improve from 0.00204\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0021 - mean_absolute_error: 0.0353 - learning_rate: 1.6000e-06\n",
      "Epoch 68/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0022 - mean_absolute_error: 0.0348\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "\n",
      "Epoch 68: loss did not improve from 0.00204\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0022 - mean_absolute_error: 0.0349 - learning_rate: 1.6000e-06\n",
      "Epoch 68: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d7c637dc10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X, y, epochs=100, batch_size=32, callbacks=[es, rlr, mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>16.190001</td>\n",
       "      <td>16.549999</td>\n",
       "      <td>16.190001</td>\n",
       "      <td>16.549999</td>\n",
       "      <td>16.516966</td>\n",
       "      <td>33461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>16.490000</td>\n",
       "      <td>16.719999</td>\n",
       "      <td>16.370001</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>16.666668</td>\n",
       "      <td>55940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>16.780001</td>\n",
       "      <td>16.959999</td>\n",
       "      <td>16.620001</td>\n",
       "      <td>16.730000</td>\n",
       "      <td>16.696608</td>\n",
       "      <td>37064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>16.570000</td>\n",
       "      <td>16.830000</td>\n",
       "      <td>16.796408</td>\n",
       "      <td>26958200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>16.740000</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.709999</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.996010</td>\n",
       "      <td>28400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>16.959999</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.996010</td>\n",
       "      <td>35070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>16.920000</td>\n",
       "      <td>17.049999</td>\n",
       "      <td>16.770000</td>\n",
       "      <td>16.799999</td>\n",
       "      <td>16.766466</td>\n",
       "      <td>28547700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>16.879999</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>16.840000</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>17.215569</td>\n",
       "      <td>37921500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>17.040001</td>\n",
       "      <td>17.410000</td>\n",
       "      <td>17.020000</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>17.265469</td>\n",
       "      <td>45912100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>17.440001</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>17.315371</td>\n",
       "      <td>28945400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>17.840000</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>17.650000</td>\n",
       "      <td>17.614771</td>\n",
       "      <td>58618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>17.920000</td>\n",
       "      <td>18.360001</td>\n",
       "      <td>17.809999</td>\n",
       "      <td>18.360001</td>\n",
       "      <td>18.323355</td>\n",
       "      <td>58488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>18.350000</td>\n",
       "      <td>18.530001</td>\n",
       "      <td>17.930000</td>\n",
       "      <td>18.219999</td>\n",
       "      <td>18.183632</td>\n",
       "      <td>48575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>18.309999</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>18.030001</td>\n",
       "      <td>18.260000</td>\n",
       "      <td>18.223553</td>\n",
       "      <td>33470200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>18.260000</td>\n",
       "      <td>18.469999</td>\n",
       "      <td>18.090000</td>\n",
       "      <td>18.469999</td>\n",
       "      <td>18.433134</td>\n",
       "      <td>33920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>18.459999</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.240000</td>\n",
       "      <td>18.203592</td>\n",
       "      <td>35567700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>19.629999</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>89768200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>19.620001</td>\n",
       "      <td>19.980000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.890221</td>\n",
       "      <td>81989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>19.670000</td>\n",
       "      <td>20.049999</td>\n",
       "      <td>19.570000</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>19.810381</td>\n",
       "      <td>55726200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.360001</td>\n",
       "      <td>19.490000</td>\n",
       "      <td>19.451097</td>\n",
       "      <td>46203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>19.740000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.680000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>19.660681</td>\n",
       "      <td>41576600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Open       High        Low      Close  Adj Close  \\\n",
       "0   2018-01-02  16.190001  16.549999  16.190001  16.549999  16.516966   \n",
       "1   2018-01-03  16.490000  16.719999  16.370001  16.700001  16.666668   \n",
       "2   2018-01-04  16.780001  16.959999  16.620001  16.730000  16.696608   \n",
       "3   2018-01-05  16.700001  16.860001  16.570000  16.830000  16.796408   \n",
       "4   2018-01-08  16.740000  17.030001  16.709999  17.030001  16.996010   \n",
       "5   2018-01-09  17.030001  17.160000  16.959999  17.030001  16.996010   \n",
       "6   2018-01-10  16.920000  17.049999  16.770000  16.799999  16.766466   \n",
       "7   2018-01-11  16.879999  17.299999  16.840000  17.250000  17.215569   \n",
       "8   2018-01-12  17.040001  17.410000  17.020000  17.299999  17.265469   \n",
       "9   2018-01-15  17.320000  17.440001  17.150000  17.350000  17.315371   \n",
       "10  2018-01-16  17.350000  17.840000  17.299999  17.650000  17.614771   \n",
       "11  2018-01-17  17.920000  18.360001  17.809999  18.360001  18.323355   \n",
       "12  2018-01-18  18.350000  18.530001  17.930000  18.219999  18.183632   \n",
       "13  2018-01-19  18.309999  18.420000  18.030001  18.260000  18.223553   \n",
       "14  2018-01-22  18.260000  18.469999  18.090000  18.469999  18.433134   \n",
       "15  2018-01-23  18.400000  18.459999  18.000000  18.240000  18.203592   \n",
       "16  2018-01-24  18.420000  19.629999  18.420000  19.340000  19.301397   \n",
       "17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       "18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       "19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       "20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       "21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       "\n",
       "      Volume  \n",
       "0   33461800  \n",
       "1   55940900  \n",
       "2   37064900  \n",
       "3   26958200  \n",
       "4   28400000  \n",
       "5   35070900  \n",
       "6   28547700  \n",
       "7   37921500  \n",
       "8   45912100  \n",
       "9   28945400  \n",
       "10  58618300  \n",
       "11  58488900  \n",
       "12  48575800  \n",
       "13  33470200  \n",
       "14  33920000  \n",
       "15  35567700  \n",
       "16  89768200  \n",
       "17         0  \n",
       "18  81989500  \n",
       "19  55726200  \n",
       "20  46203000  \n",
       "21  41576600  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_teste = pd.read_csv('datasets/petr4_teste.csv')\n",
    "base_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.190001],\n",
       "       [16.49    ],\n",
       "       [16.780001],\n",
       "       [16.700001],\n",
       "       [16.74    ],\n",
       "       [17.030001],\n",
       "       [16.92    ],\n",
       "       [16.879999],\n",
       "       [17.040001],\n",
       "       [17.32    ],\n",
       "       [17.35    ],\n",
       "       [17.92    ],\n",
       "       [18.35    ],\n",
       "       [18.309999],\n",
       "       [18.26    ],\n",
       "       [18.4     ],\n",
       "       [18.42    ],\n",
       "       [19.34    ],\n",
       "       [19.620001],\n",
       "       [19.67    ],\n",
       "       [19.77    ],\n",
       "       [19.74    ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_teste = base_teste.iloc[: , 1:2].values\n",
    "y_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [base, base_teste]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[            Date       Open       High        Low      Close  Adj Close  \\\n",
       " 0     2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       " 1     2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       " 2     2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       " 3     2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       " 4     2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       " ...          ...        ...        ...        ...        ...        ...   \n",
       " 1240  2017-12-25  15.750000  15.750000  15.750000  15.750000  15.718563   \n",
       " 1241  2017-12-26  15.750000  15.990000  15.690000  15.970000  15.938125   \n",
       " 1242  2017-12-27  15.990000  16.139999  15.980000  16.049999  16.017963   \n",
       " 1243  2017-12-28  16.100000  16.129999  16.000000  16.100000  16.067865   \n",
       " 1244  2017-12-29  16.100000  16.100000  16.100000  16.100000  16.067865   \n",
       " \n",
       "           Volume  \n",
       " 0     30182600.0  \n",
       " 1     30552600.0  \n",
       " 2     36141000.0  \n",
       " 3     28069600.0  \n",
       " 4     29091300.0  \n",
       " ...          ...  \n",
       " 1240         0.0  \n",
       " 1241  22173100.0  \n",
       " 1242  23552200.0  \n",
       " 1243  19011500.0  \n",
       " 1244         0.0  \n",
       " \n",
       " [1242 rows x 7 columns],\n",
       "           Date       Open       High        Low      Close  Adj Close  \\\n",
       " 0   2018-01-02  16.190001  16.549999  16.190001  16.549999  16.516966   \n",
       " 1   2018-01-03  16.490000  16.719999  16.370001  16.700001  16.666668   \n",
       " 2   2018-01-04  16.780001  16.959999  16.620001  16.730000  16.696608   \n",
       " 3   2018-01-05  16.700001  16.860001  16.570000  16.830000  16.796408   \n",
       " 4   2018-01-08  16.740000  17.030001  16.709999  17.030001  16.996010   \n",
       " 5   2018-01-09  17.030001  17.160000  16.959999  17.030001  16.996010   \n",
       " 6   2018-01-10  16.920000  17.049999  16.770000  16.799999  16.766466   \n",
       " 7   2018-01-11  16.879999  17.299999  16.840000  17.250000  17.215569   \n",
       " 8   2018-01-12  17.040001  17.410000  17.020000  17.299999  17.265469   \n",
       " 9   2018-01-15  17.320000  17.440001  17.150000  17.350000  17.315371   \n",
       " 10  2018-01-16  17.350000  17.840000  17.299999  17.650000  17.614771   \n",
       " 11  2018-01-17  17.920000  18.360001  17.809999  18.360001  18.323355   \n",
       " 12  2018-01-18  18.350000  18.530001  17.930000  18.219999  18.183632   \n",
       " 13  2018-01-19  18.309999  18.420000  18.030001  18.260000  18.223553   \n",
       " 14  2018-01-22  18.260000  18.469999  18.090000  18.469999  18.433134   \n",
       " 15  2018-01-23  18.400000  18.459999  18.000000  18.240000  18.203592   \n",
       " 16  2018-01-24  18.420000  19.629999  18.420000  19.340000  19.301397   \n",
       " 17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       " 18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       " 19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       " 20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       " 21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       " \n",
       "       Volume  \n",
       " 0   33461800  \n",
       " 1   55940900  \n",
       " 2   37064900  \n",
       " 3   26958200  \n",
       " 4   28400000  \n",
       " 5   35070900  \n",
       " 6   28547700  \n",
       " 7   37921500  \n",
       " 8   45912100  \n",
       " 9   28945400  \n",
       " 10  58618300  \n",
       " 11  58488900  \n",
       " 12  48575800  \n",
       " 13  33470200  \n",
       " 14  33920000  \n",
       " 15  35567700  \n",
       " 16  89768200  \n",
       " 17         0  \n",
       " 18  81989500  \n",
       " 19  55726200  \n",
       " 20  46203000  \n",
       " 21  41576600  ]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>19.620001</td>\n",
       "      <td>19.980000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.890221</td>\n",
       "      <td>81989500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>19.670000</td>\n",
       "      <td>20.049999</td>\n",
       "      <td>19.570000</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>19.810381</td>\n",
       "      <td>55726200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.360001</td>\n",
       "      <td>19.490000</td>\n",
       "      <td>19.451097</td>\n",
       "      <td>46203000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>19.740000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.680000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>19.660681</td>\n",
       "      <td>41576600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1264 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Open       High        Low      Close  Adj Close  \\\n",
       "0   2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1   2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2   2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3   2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4   2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       "18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       "19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       "20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       "21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       "\n",
       "        Volume  \n",
       "0   30182600.0  \n",
       "1   30552600.0  \n",
       "2   36141000.0  \n",
       "3   28069600.0  \n",
       "4   29091300.0  \n",
       "..         ...  \n",
       "17         0.0  \n",
       "18  81989500.0  \n",
       "19  55726200.0  \n",
       "20  46203000.0  \n",
       "21  41576600.0  \n",
       "\n",
       "[1264 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_completa = pd.concat(frames)\n",
    "base_completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_completa = base_completa.drop(['Date'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas = base_completa[len(base_completa) - len(base_teste) - 90:].values # 1264 - 22 - 90 = 1152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas = normalizador.transform(entradas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_teste = []\n",
    "for i in range(90,112):\n",
    "    X_teste.append(entradas[i-90: i,0:6])\n",
    "X_teste = np.array(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 90, 6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 600ms/step\n"
     ]
    }
   ],
   "source": [
    "previsoes = regressor.predict(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5761888 ],\n",
       "       [0.5825473 ],\n",
       "       [0.5884881 ],\n",
       "       [0.5944003 ],\n",
       "       [0.60019684],\n",
       "       [0.60591084],\n",
       "       [0.6115284 ],\n",
       "       [0.616186  ],\n",
       "       [0.6199924 ],\n",
       "       [0.6232841 ],\n",
       "       [0.62658495],\n",
       "       [0.63034606],\n",
       "       [0.6361804 ],\n",
       "       [0.6442868 ],\n",
       "       [0.65372676],\n",
       "       [0.66338485],\n",
       "       [0.67176026],\n",
       "       [0.6797642 ],\n",
       "       [0.6894345 ],\n",
       "       [0.70098245],\n",
       "       [0.71356654],\n",
       "       [0.7249609 ]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = normalizador_previsao.inverse_transform(previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.092537],\n",
       "       [16.223776],\n",
       "       [16.346394],\n",
       "       [16.468422],\n",
       "       [16.588062],\n",
       "       [16.706   ],\n",
       "       [16.821945],\n",
       "       [16.91808 ],\n",
       "       [16.996643],\n",
       "       [17.064583],\n",
       "       [17.132713],\n",
       "       [17.210342],\n",
       "       [17.330763],\n",
       "       [17.49808 ],\n",
       "       [17.69292 ],\n",
       "       [17.892263],\n",
       "       [18.065132],\n",
       "       [18.230333],\n",
       "       [18.429928],\n",
       "       [18.668278],\n",
       "       [18.928013],\n",
       "       [19.163193]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.190001],\n",
       "       [16.49    ],\n",
       "       [16.780001],\n",
       "       [16.700001],\n",
       "       [16.74    ],\n",
       "       [17.030001],\n",
       "       [16.92    ],\n",
       "       [16.879999],\n",
       "       [17.040001],\n",
       "       [17.32    ],\n",
       "       [17.35    ],\n",
       "       [17.92    ],\n",
       "       [18.35    ],\n",
       "       [18.309999],\n",
       "       [18.26    ],\n",
       "       [18.4     ],\n",
       "       [18.42    ],\n",
       "       [19.34    ],\n",
       "       [19.620001],\n",
       "       [19.67    ],\n",
       "       [19.77    ],\n",
       "       [19.74    ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.384926, 17.87454563636364)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes.mean(), y_teste.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4930802592995383"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_teste, color = \"red\", label = \"Preço real\")\n",
    "plt.plot(previsoes, color = \"blue\", label = \"Previsões\")\n",
    "plt.title(\"Previsão do preço das ações\")\n",
    "plt.xlabel(\"tempo\")\n",
    "plt.ylabel(\"Valor\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
